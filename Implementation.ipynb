{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем веса для VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c --no-check-certificate https://bethgelab.org/media/uploads/pytorch_models/vgg_conv.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as tutils\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оригинальная архитектура сети VGG19 представлена 16 свёрточными слоями в 5 блоках свёртки, после каждого из которых применяется MaxPooling. В то же время Гатис и др. в своей статье отмечают, что использование AvgPooling вместо MaxPooling улучшает градиент и делает результирующие изображения более привлекательными с точки зрения восприятия человеком.\n",
    "\n",
    "На разных этапах свёртки информация о входном изображении отличается, увеличивается число накопленных признаков благодаря росту количества применённых фильтров. Одновременно с этим уменьшается разрешение самого изображения после очередного применения downsampling-механизма.\n",
    "\n",
    "Авторы статьи предприняли попытку визуализировать накопленную информацию на разных слоях CNN. Входное изображение было по очереди воссоздано из первого слоя каждого свёрточного блока. В результате удалось выяснить, что реконструированное изображение с первых слоёв первых трёх блоков почти идентично исходному, а далее информация об отдельных пикселях начинает теряться, но при этом сохраняется \"высокоуровневая\" информация об объектах на изображении, т.е. об их форме, взаимном расположении и т.п.\n",
    "\n",
    "Кроме того авторы статьи предприняли различные попытки воссоздания стиля изображения. Для сохранения информации о стиле считалась корреляция между всеми выявленными признаками, которые были найдены фильтрами на разных слоях CNN. Далее были использованы 5 различных наборов корреляций, полученных соответственно с первых слоёв следующих свёрточных блоков: 1; 1 и 2; 1-3; 1-4; 1-5. Авторам удалось выяснить, что использование нового дополнительного слоя для реконструкции стиля постепенно увеличивает масштаб отрисовки отдельного признака, при этом информация о взаимном расположении данных признаков постепенно утрачивается.\n",
    "\n",
    "Таким образом самым рациональным подходом для Style Transfer алгоритма будет использование карты признаков одного из \"верхних\" слоёв CNN для переноса контента на результирующее изображение, и использование нескольких \"глубоких\" слоёв CNN для переноса стиля. Авторами статьи были использованы первые слои всех пяти блоков для сохранения стиля и один слой четвёртого блока для переноса контента.\n",
    "\n",
    "Для повторения некоторых экспериментов, описанных и проведённых Гатисом и др., определим параметризованный конструктор, для возможности создания сетей с различным pooling-типом и параметризованную функцию forward(), чтобы иметь возможность определять выходы каких слоёв мы будем использовать при переносе стиля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_nst(nn.Module):\n",
    "    def __init__(self, pooling=None):\n",
    "        super(VGG_nst,self).__init__()\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        if pooling is 'avg':\n",
    "            self.pool_1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        else:\n",
    "            self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool_5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        \n",
    "    def forward(self, x, out_layers):\n",
    "        out = {}\n",
    "        out['conv1_1'] = F.relu(self.conv1_1(x))\n",
    "        out['conv1_2'] = F.relu(self.conv1_2(out['conv1_1']))\n",
    "        out['pool_1'] = self.pool_1(out['conv1_2'])\n",
    "        \n",
    "        out['conv2_1'] = F.relu(self.conv2_1(out['pool_1']))\n",
    "        out['conv2_2'] = F.relu(self.conv2_2(out['conv2_1']))\n",
    "        out['pool_2'] = self.pool_2(out['conv2_2'])\n",
    "        \n",
    "        out['conv3_1'] = F.relu(self.conv3_1(out['pool_2']))\n",
    "        out['conv3_2'] = F.relu(self.conv3_2(out['conv3_1']))\n",
    "        out['conv3_3'] = F.relu(self.conv3_3(out['conv3_2']))\n",
    "        out['conv3_4'] = F.relu(self.conv3_4(out['conv3_3']))\n",
    "        out['pool_3'] = self.pool_3(out['conv3_4'])\n",
    "        \n",
    "        out['conv4_1'] = F.relu(self.conv4_1(out['pool_3']))\n",
    "        out['conv4_2'] = F.relu(self.conv4_2(out['conv4_1']))\n",
    "        out['conv4_3'] = F.relu(self.conv4_3(out['conv4_2']))\n",
    "        out['conv4_4'] = F.relu(self.conv4_4(out['conv4_3']))\n",
    "        out['pool_4'] = self.pool_4(out['conv4_4'])\n",
    "        \n",
    "        out['conv5_1'] = F.relu(self.conv5_1(out['pool_4']))\n",
    "        out['conv5_2'] = F.relu(self.conv5_2(out['conv5_1']))\n",
    "        out['conv5_3'] = F.relu(self.conv5_3(out['conv5_2']))\n",
    "        out['conv5_4'] = F.relu(self.conv5_4(out['conv5_3']))\n",
    "        out['pool_5'] = self.pool_5(out['conv5_4'])\n",
    "\n",
    "        return [out[layer] for layer in out_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b, c, h*w) #bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)\n",
    "        # batch1 : bxmxp, batch2 : bxpxn -> bxmxn\n",
    "        G = torch.bmm(f, f.transpose(1, 2)) # f: BxCx(HxW), f.transpose: Bx(HxW)xC -> BxCxC\n",
    "        return G.div_(h*w)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        GramInput = GramMatrix()(input)\n",
    "        return nn.MSELoss()(GramInput, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size):\n",
    "    img = transforms.Resize(size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = transforms.Lambda(lambda x:x[torch.LongTensor([2, 1, 0])])(img) #RGB to BGR\n",
    "    img = transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], std=[1, 1, 1])(img) #subracting imagenet mean\n",
    "    #img = transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], std=[0.225, 0.224, 0.229])(img) #subracting imagenet mean\n",
    "    img = transforms.Lambda(lambda x: x.mul_(255))(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def postprocess(img):\n",
    "    img = transforms.Lambda(lambda x: x.mul_(1./255))(img)\n",
    "    img = transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], std=[1,1,1])(img)\n",
    "    img = transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])])(img) #turn to RGB\n",
    "    img = img.clamp_(0,1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_img(path, img_size):\n",
    "    img = Image.open(path)\n",
    "    img = preprocess(img, img_size)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img.to(device)\n",
    "\n",
    "\n",
    "def load_raw_img(path):\n",
    "    image = Image.open(path)\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "    return image_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_preview(tensor):\n",
    "    image_tensor = tensor.cpu().clone()\n",
    "    image = transforms.ToPILImage()(image_tensor.squeeze(0))\n",
    "    image = transforms.Resize(imsize)(image)\n",
    "    image = transforms.CenterCrop(imsize)(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_intermediate_results(content, style, output):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(get_preview(content))\n",
    "    plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    plt.title(\"Content Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(get_preview(style))\n",
    "    plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    plt.title(\"Style Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(get_preview(output))\n",
    "    plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    plt.title(\"Output Image\")\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(model, optim_img, optimizer, iter_num, loss_layers, targets, loss_funcs, weights, verbose=True):\n",
    "    if verbose:\n",
    "        style_prev = load_raw_img(style_path)\n",
    "        content_prev = load_raw_img(content_path)\n",
    "    #history = []\n",
    "    for iteration in tqdm(range(iter_num)):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = model(optim_img, loss_layers)\n",
    "            totalLossList = []\n",
    "            for i in range(len(out)):\n",
    "                layer_output = out[i]\n",
    "                loss_i = loss_funcs[i]\n",
    "                target_i = targets[i]\n",
    "                totalLossList.append(loss_i(layer_output, target_i) * weights[i])\n",
    "                #history.append((loss_i(layer_output, target_i) * weights[i]).item())\n",
    "            total_loss = sum(totalLossList)\n",
    "            total_loss.backward()\n",
    "            #history.append(total_loss.item())\n",
    "            return total_loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        if iteration % 5 == 0 and verbose:\n",
    "            int_result = postprocess(optim_img.data[0].cpu().squeeze())\n",
    "            show_intermediate_results(content_prev, style_prev, int_result)\n",
    "    out_img = optim_img.data[0].cpu().squeeze()\n",
    "    res_img = postprocess(out_img)\n",
    "    return #history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path_to_pretrained, pooling='avg'):\n",
    "    if pooling != 'avg' and pooling != 'max':\n",
    "        raise BaseException(\"Неправильно указан pooling-тип. \" +\n",
    "                            \"Допустимые значения: avg, max.\")\n",
    "    model = VGG_nst(pooling)\n",
    "    model.load_state_dict(torch.load(path_to_pretrained))\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def get_loss_funcs(style_layers, content_layers):\n",
    "    style_losses = [StyleLoss()] * len(style_layers)\n",
    "    content_losses = [nn.MSELoss()] * len(content_layers)\n",
    "    funcs = style_losses + content_losses\n",
    "    funcs = [f.to(device) for f in funcs]\n",
    "    return funcs\n",
    "\n",
    "\n",
    "def get_targets(model, style_layers, content_layers, style_image, content_image):\n",
    "    style_targets = [GramMatrix()(t).detach() for t in model(style_image, style_layers)]\n",
    "    content_targets = [t.detach() for t in model(content_image, content_layers)]\n",
    "    targets = style_targets + content_targets\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_path = \"/content/vangogh_starry_night.jpg\"\n",
    "content_path = \"/content/balley.png\"\n",
    "\n",
    "vgg_directory = \"/content/vgg_conv.pth\"\n",
    "\n",
    "style_layers = ['conv1_1','conv2_1','conv3_1','conv4_1','conv5_1']\n",
    "content_layers = ['conv4_2']\n",
    "\n",
    "imsize = 512\n",
    "style_image = load_img(style_path, imsize)\n",
    "content_image = load_img(content_path, imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nst_model = get_model(path_to_pretrained=vgg_directory, pooling='avg')\n",
    "targets = get_targets(nst_model, style_layers, content_layers, style_image, content_image)\n",
    "loss_funcs = get_loss_funcs(style_layers, content_layers)\n",
    "loss_layers = style_layers + content_layers\n",
    "\n",
    "style_weight = 1e+3  # 1000\n",
    "content_weight = 1   # 5\n",
    "weights = [style_weight] * len(style_layers) + [content_weight] * len(content_layers)\n",
    "\n",
    "optimImg = Variable(content_image.data.clone(), requires_grad=True).to(device)\n",
    "optimizer = optim.LBFGS([optimImg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_style_transfer(model = nst_model,\n",
    "                   optim_img = optimImg,\n",
    "                   optimizer = optimizer,\n",
    "                   iter_num = 40,\n",
    "                   loss_layers = loss_layers,\n",
    "                   targets = targets,\n",
    "                   loss_funcs = loss_funcs,\n",
    "                   weights = weights)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}